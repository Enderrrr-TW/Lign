{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set gpu if available\n",
    "if th.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    #device = th.device(\"cuda\")\n",
    "    device = th.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operation for neigbors\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "    \n",
    "#gcn layer in network\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.pull(g.nodes())\n",
    "        g.apply_nodes(self.apply_mod)\n",
    "        \n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "#network\n",
    "class LIGN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(LIGN, self).__init__()\n",
    "        self.gcn1 = GCN(in_feats, 100, F.relu)\n",
    "        self.gcn2 = GCN(100, 30, F.relu)\n",
    "        self.gcn3 = GCN(30, out_feats, th.tanh)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        \n",
    "        return self.gcn3(g, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "\n",
    "def similarity_matrix(x): #pairwise distance\n",
    "    \"\"\"x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y = x\n",
    "    y_norm = x_norm.view(1, -1)\n",
    "\n",
    "    dist = x_norm + y_norm - 2.0 * th.mm(x, th.transpose(y, 0, 1))\"\"\"\n",
    "    n = x.size(0)\n",
    "    d = x.size(1)\n",
    "\n",
    "    y = x.unsqueeze(0).expand(n, n, d)\n",
    "    x = x.unsqueeze(1).expand(n, n, d)\n",
    "    \n",
    "    dist = th.pow(x - y, 2).sum(2)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def same_label(y):\n",
    "    s = y.size(0)\n",
    "    y_expand = y.unsqueeze(0).expand(s, s)\n",
    "    Y = y_expand.eq(y_expand.t())\n",
    "    return Y\n",
    "\n",
    "def pairwaise_loss(output, labels):\n",
    "    \"\"\"\n",
    "    if nodes with the same label: x^2\n",
    "    if nodes with different label: -x^2\n",
    "    \"\"\"\n",
    "    sim = similarity_matrix(output)\n",
    "    temp = same_label(labels)\n",
    "    temp_inv = temp*(-1) + 1\n",
    "    \n",
    "    same_l = (temp * sim)\n",
    "    same_l_inv = (temp_inv * sim)\n",
    "    loss = (th.sum(same_l)/th.sum(temp))**2 - (th.sum(same_l_inv)/th.sum(temp_inv))**2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def has_1_rand_label(nodes): return (nodes.data['t_labels'] == possible_lab[0]).squeeze(0)\n",
    "\n",
    "def filter_knn_label(nodes): return (nodes.data['t_labels'] == curr_used_lab).squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load CORA dataset\n",
    "data = citegrh.load_cora()\n",
    "ds_features = th.FloatTensor(data.features).to(device) #convert to pytorch data type and add to cpu/gpu\n",
    "ds_labels = th.LongTensor(data.labels).to(device)\n",
    "ds_g = data.graph\n",
    "\n",
    "# add self loop for the sum of festures\n",
    "ds_g.remove_edges_from(nx.selfloop_edges(ds_g))\n",
    "ds_g = DGLGraph(ds_g)\n",
    "ds_g.add_edges(ds_g.nodes(), ds_g.nodes())\n",
    "ds_g.ndata['features'] = ds_features\n",
    "ds_g.ndata['t_labels'] = ds_labels #used to filter and train the first two labels, not needed for prediction\n",
    "\n",
    "# to coordinate sending of features over the graph network\n",
    "m_func = fn.copy_src(src='h', out='m')\n",
    "m_reduce_func = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Create Model ############\n",
    "\n",
    "# hyper parameters\n",
    "DIST_VEC_SIZE = 10\n",
    "NUMBER_OF_LABELS = th.unique(ds_g.ndata[\"t_labels\"]).size(0)\n",
    "k_NEIGHBOR = 3\n",
    "EPOCH = 50\n",
    "INIT_NUM_LAB = 3 #must be at least 1\n",
    "Lambda = 0.0001\n",
    "\n",
    "model = LIGN(ds_features.size()[1], DIST_VEC_SIZE).to(device)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=k_NEIGHBOR)\n",
    "opt = th.optim.Adam(model.parameters(), lr=1e-3)# only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0\n",
      "> 1\n",
      "> 2\n",
      "> 3\n",
      "> 4\n",
      "> 5\n",
      "> 6\n",
      "> 7\n",
      "> 8\n",
      "> 9\n",
      "> 10\n",
      "> 11\n",
      "> 12\n",
      "> 13\n",
      "> 14\n",
      "> 15\n",
      "> 16\n",
      "> 17\n",
      "> 18\n",
      "> 19\n",
      "> 20\n",
      "> 21\n",
      "> 22\n",
      "> 23\n",
      "> 24\n",
      "> 25\n",
      "> 26\n",
      "> 27\n",
      "> 28\n",
      "> 29\n",
      "> 30\n",
      "> 31\n",
      "> 32\n",
      "> 33\n",
      "> 34\n",
      "> 35\n",
      "> 36\n",
      "> 37\n",
      "> 38\n",
      "> 39\n",
      "> 40\n",
      "> 41\n",
      "> 42\n",
      "> 43\n",
      "> 44\n",
      "> 45\n",
      "> 46\n",
      "> 47\n",
      "> 48\n",
      "> 49\n",
      "[6 4 4 5 4 5 6 6 6 5 5 6 6 4 5 4 4 5 4 5 5 5 6 5 6 6 6 5 6 4 5 4 6 6 5 5 4\n",
      " 4 4 6 6 4 6 6 4 4 4 6 5 4 5 4 4 4 6 4 6 4 5 5 4 4 5 5 6 6 6 6 5 5 6 6 4 4\n",
      " 5 4 4 5 6 6 4 6 5 6 5 6 5 6 6 4 5 4 6 5 4 5 6 4 6 4 4 4 6 4 5 4 6 6 4 6 6\n",
      " 4 6 5 4 5 6 6 4 6 6 4 6 5 5 6 5 6 6 4 6 5 4 6 6 6 6 6 6 6 6 6 5 5 6 4 4 4\n",
      " 5 6 6 5 6 4 6 6 6 4 4 6 6 6 6 6 4 5 6 4 6 5 4 4 5 5 5 5 6 6 6 6 4 6 6 6 6\n",
      " 4 4 6 6 6 6 5 6 5 4 5 4 6 6 5 4 6 6 6 6 6 6 6 6 4 6 4 5 6 5 5 4 6 4 5 6 4\n",
      " 6 6 6 5 6 5 5 6 6 5 6 4 6 6 4 4 6 6 6 5 5 4 6 4 5 6 6 5 6 5 6 6 4 6 6 4 6\n",
      " 5 4 6 4 6 6 6 6 6 5 4 6 6 6 6 6 6 6 5 4 5 6 4 4 5 4 6 5 4 4 6 6 5 6 6 4 6\n",
      " 5 5 4 5 5 4 6 4 6 6 6 4 6 6 6 6 4 4 4 6 4 4 5 6 4 5 6 4 6 6 4 6 4 4 4 6 5\n",
      " 6 4 5 4 6 5 4 5 6 5 4 5 5 6 6 6 6 5 4 4 6 4 6 4 5 4 4 4 5 5 6 5 6 4 6 6 4\n",
      " 5 6 4 6 6 6 4 6 4 6 4 5 4 6 6 6 6 4 6 6 5 6 4 5 5 5 6 6 5 5 5 6 6 6 4 5 5\n",
      " 5 4 6 5 6 4 5 6 4 6 5 5 5 6 4 5 6 5 6 4 4 6 6 6 5 5 6 4 5 6 4 6 6 5 5 6 5\n",
      " 6 6 5 4 4 4 4 6 6 5 6 6 5 6 6 6 5 5 4 4 4 4 5 6 6 6 6 4 5 4 4 4 6 5 6 6 6\n",
      " 4 5 6 6 6 5 6 4 6 6 5 5 6 6 5 6 4 4 4 6 4 6 6 5 6 4 5 6 6 4 4 5 6 4 6 6 5\n",
      " 5 4 4 6 5 4 4 5 6 6 5 4 5 6 6 5 6 4 6 4 6 4 4 4 4 5 6 6 6 6 4 4 4 6 4 4 6\n",
      " 5 5 6 6 5 4 4 5 5 6 5 6 6 6 5 6 4 4 4 6 4 6 5 4 4 4 6 4 6 4 4 6 6 4 4 6 4\n",
      " 6 4 4 6 6 6 6 6 6 4 5 4 6 5 6 4 5 6 6 4 6 5 6 6 5 5 5 6 6 5 6 6 6 6 5 4 4\n",
      " 6 5 6 5 4 6 6 6 5 6 5 4 6 4 6 6 4 5 6 6 4 6 4 6 6 6 6 6 4 4 4 4 5 6 6 4 5\n",
      " 6 4 6 6 5 6 6 6 6 6 6 4 5 6 4 6 5 4 5 4 5 6 4 4 6 6 6 4 6 5 6 6 6 4 6 4 5\n",
      " 5 6 6 6 4 5 6 6 6 6 6 5 6 4 4 6 5 4 6 6 6 6 5 5 6 6 4 5 5 6 5 6 6 6 4 6 5\n",
      " 6 6 4 6 4 6 6 5]\n",
      "[6 4 4 5 4 5 6 6 5 5 5 6 5 4 5 4 4 5 4 5 5 5 6 5 6 6 6 5 6 4 5 4 5 6 5 6 4\n",
      " 4 4 6 6 4 6 6 4 4 4 6 5 6 5 4 4 4 6 4 6 4 5 5 4 4 5 5 6 6 6 6 5 5 6 6 4 4\n",
      " 5 4 4 5 6 6 4 6 5 6 5 6 5 6 6 4 5 4 6 5 4 5 6 4 6 4 4 4 6 4 5 4 6 6 4 6 6\n",
      " 4 6 5 4 5 5 6 4 6 6 4 6 5 5 6 5 6 6 4 6 5 4 6 6 6 6 6 6 4 6 6 5 5 6 4 4 4\n",
      " 5 6 6 5 5 4 6 6 6 4 4 6 6 6 6 6 4 5 6 4 6 5 4 4 5 5 5 5 6 6 6 6 4 6 6 6 6\n",
      " 4 4 6 6 6 6 5 6 5 4 5 4 6 6 5 4 6 6 6 6 6 6 6 6 4 6 4 5 6 6 5 4 6 4 5 6 4\n",
      " 6 6 6 5 5 5 5 6 6 5 6 4 6 6 4 4 6 6 6 5 5 4 6 4 5 6 4 5 6 5 6 6 4 6 6 4 5\n",
      " 5 4 6 4 6 6 5 6 6 5 4 6 6 6 6 6 6 6 5 4 5 6 4 4 5 4 5 5 4 4 6 6 5 6 6 4 6\n",
      " 5 5 4 5 5 4 6 4 6 6 6 4 6 6 6 6 4 4 4 6 4 4 5 6 4 5 6 4 6 6 4 6 4 4 4 6 5\n",
      " 5 6 6 4 6 5 4 5 6 5 4 5 6 6 6 5 6 5 4 4 6 4 6 4 5 4 4 4 5 5 5 5 6 4 6 6 4\n",
      " 5 6 4 6 6 6 4 6 4 6 4 5 4 6 6 6 6 4 6 6 5 6 4 5 5 5 6 6 5 5 5 6 6 6 4 5 5\n",
      " 5 4 6 5 6 4 5 6 4 6 5 5 6 6 4 5 5 5 6 4 4 6 6 6 5 5 6 4 5 6 4 5 6 6 5 6 5\n",
      " 4 6 5 4 4 6 4 6 6 5 6 4 5 6 6 6 5 5 4 4 4 4 5 6 6 6 6 4 5 4 4 4 6 5 6 6 6\n",
      " 4 5 6 4 6 5 6 4 6 6 5 5 6 6 5 6 6 4 4 6 4 6 6 5 6 4 5 6 5 4 4 5 6 4 6 6 5\n",
      " 5 4 4 6 6 4 4 5 6 4 5 4 5 6 6 5 6 4 6 4 6 4 4 4 4 5 6 6 6 6 4 4 4 6 4 4 6\n",
      " 5 5 6 6 5 4 4 5 5 6 5 6 6 5 5 5 4 4 4 5 4 6 5 4 4 4 6 4 6 4 4 6 6 4 4 6 4\n",
      " 6 4 4 6 6 6 6 6 6 4 5 4 6 5 6 4 5 6 6 4 6 5 6 6 5 6 5 6 6 6 6 6 6 6 5 4 4\n",
      " 6 5 6 5 4 6 6 6 5 6 5 4 6 4 6 5 4 5 6 6 4 6 4 6 6 6 6 5 4 4 4 4 5 6 6 4 5\n",
      " 6 4 6 6 5 6 5 6 5 6 6 4 6 6 4 6 5 4 5 4 5 5 4 4 6 6 6 4 6 5 6 6 6 4 6 4 5\n",
      " 5 6 6 6 4 5 6 6 6 6 6 5 6 4 4 6 5 6 6 6 6 6 5 5 6 6 4 5 5 6 5 6 6 5 4 6 5\n",
      " 6 5 4 6 4 5 6 5]\n",
      "> Initial training with 3 labels (distinction efficiency): 0.9371657754010695\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "train_g = ds_g.subgraph(ds_g.nodes()[:int(len(ds_g) * 1)])#100 percent of all the nodes\n",
    "train_g.copy_from_parent()\n",
    "\n",
    "labeled_nodes = th.LongTensor([])\n",
    "for i in range(NUMBER_OF_LABELS):\n",
    "    def has_feature_one(nodes): return (nodes.data['t_labels'] == i).squeeze(0)\n",
    "    \n",
    "    filt = train_g.filter_nodes(has_feature_one)\n",
    "    labeled_nodes = th.cat((filt[:k_NEIGHBOR+10], labeled_nodes), -1)\n",
    "    \n",
    "\n",
    "labeled_g = train_g.subgraph(labeled_nodes) #labels nodes in \n",
    "labeled_g.copy_from_parent()\n",
    "\n",
    "\"\"\"#both dataset should include all labels\n",
    "while th.unique(train_g.ndata[\"t_labels\"]).size(0) != th.unique(labeled_g.ndata[\"t_labels\"]).size(0):\n",
    "    c=th.randperm(len(ds_g.nodes())) #shuffle\n",
    "    ds_g.nodes =ds_g.nodes()[c]\n",
    "    \n",
    "    train_g = ds_g.subgraph(ds_g.nodes()[:int(len(ds_g) * 1)])#100 percent of all the nodes\n",
    "    labeled_g = ds_g.subgraph(ds_g.nodes()[int(len(ds_g) * .20):]) #20 percent labeled nodes for knn\n",
    "    train_g.copy_from_parent()\n",
    "    labeled_g.copy_from_parent()\"\"\"\n",
    "\n",
    "\n",
    "possible_lab = list(range(NUMBER_OF_LABELS))\n",
    "rand.shuffle(possible_lab) #set random order for how labels are added\n",
    "\n",
    "used_lab = []\n",
    "curr_used_lab = 0\n",
    "\n",
    "EPOCH_SIZE = 0\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "#############train with 2 labels of 7 labels (uses true label from the dataset), assign vector to each node,\n",
    "            #knn (1 labeled node for each label), test\n",
    "selected_nodes = train_g.filter_nodes(has_1_rand_label) #add two unseen labels\n",
    "used_lab.append(possible_lab.pop(0))\n",
    "for i in range(INIT_NUM_LAB - 1):\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "\n",
    "c=th.randperm(len(selected_nodes)) #shuffle\n",
    "selected_nodes=selected_nodes[c]\n",
    "\n",
    "EPOCH_SIZE = int(len(selected_nodes)*.80)\n",
    "\n",
    "#train\n",
    "addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer \n",
    "for epoch in range(EPOCH):\n",
    "    print(\"> \" + str(epoch))\n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    epoch_nodes = selected_nodes[:EPOCH_SIZE] #selected 20% of random nodes to train with at each epoch\n",
    "    \n",
    "    error = []\n",
    "    for count in range(int(len(epoch_nodes)*.7), len(epoch_nodes)):\n",
    "        sub_graph = train_g.subgraph(epoch_nodes[:count])\n",
    "        sub_graph.copy_from_parent()\n",
    "        sub_graph.register_message_func(m_func)\n",
    "        sub_graph.register_reduce_func(m_reduce_func)\n",
    "        \n",
    "        feats = sub_graph.ndata['features']\n",
    "        labs = sub_graph.ndata['t_labels'] #true label\n",
    "        \n",
    "        out = model(sub_graph, feats)\n",
    "        \n",
    "        out2 = addon_layer(sub_graph, out) #extra layer\n",
    "        out2 = th.log_softmax(out2, 1)\n",
    "        \n",
    "        loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "        error.append(loss.item())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    #print(np.mean(error))\n",
    "\n",
    "    \n",
    "#knn and assign vector\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy())\n",
    "print((sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())\n",
    "\n",
    "#test\n",
    "print(f\"> Initial training with {INIT_NUM_LAB} labels (distinction efficiency): {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After new label added and before retraining (total: 4), accuracy of all nodes: 0.5951972555746141\n",
      "> 0\n",
      "> 1\n",
      "> 2\n",
      "> 3\n",
      "> 4\n",
      "> 5\n",
      "> 6\n",
      "> 7\n",
      "> 8\n",
      "> 9\n",
      "> 10\n",
      "> 11\n",
      "> 12\n",
      "> 13\n",
      "> 14\n",
      "> 15\n",
      "> 16\n",
      "> 17\n",
      "> 18\n",
      "> 19\n",
      "> 20\n",
      "> 21\n",
      "> 22\n",
      "> 23\n",
      "> 24\n",
      "> 25\n",
      "> 26\n",
      "> 27\n",
      "> 28\n",
      "> 29\n",
      "> 30\n",
      "> 31\n",
      "> 32\n",
      "> 33\n",
      "> 34\n",
      "> 35\n",
      "> 36\n",
      "> 37\n",
      "> 38\n",
      "> 39\n",
      "> 40\n",
      "> 41\n",
      "> 42\n",
      "> 43\n",
      "> 44\n",
      "> 45\n",
      "> 46\n",
      "> 47\n",
      "> 48\n",
      "> 49\n",
      "After new label added and before retraining (total: 5), accuracy of all nodes: 0.43027638190954776\n",
      "> 0\n",
      "> 1\n",
      "> 2\n",
      "> 3\n",
      "> 4\n",
      "> 5\n",
      "> 6\n",
      "> 7\n",
      "> 8\n",
      "> 9\n",
      "> 10\n",
      "> 11\n",
      "> 12\n",
      "> 13\n",
      "> 14\n",
      "> 15\n",
      "> 16\n",
      "> 17\n",
      "> 18\n",
      "> 19\n",
      "> 20\n",
      "> 21\n",
      "> 22\n",
      "> 23\n",
      "> 24\n",
      "> 25\n",
      "> 26\n",
      "> 27\n",
      "> 28\n",
      "> 29\n",
      "> 30\n",
      "> 31\n",
      "> 32\n",
      "> 33\n",
      "> 34\n",
      "> 35\n",
      "> 36\n",
      "> 37\n",
      "> 38\n",
      "> 39\n",
      "> 40\n",
      "> 41\n",
      "> 42\n",
      "> 43\n",
      "> 44\n",
      "> 45\n",
      "> 46\n",
      "> 47\n",
      "> 48\n",
      "> 49\n",
      "After new label added and before retraining (total: 6), accuracy of all nodes: 0.23319502074688797\n",
      "> 0\n",
      "> 1\n",
      "> 2\n",
      "> 3\n",
      "> 4\n",
      "> 5\n",
      "> 6\n",
      "> 7\n",
      "> 8\n",
      "> 9\n",
      "> 10\n",
      "> 11\n",
      "> 12\n",
      "> 13\n",
      "> 14\n",
      "> 15\n",
      "> 16\n",
      "> 17\n",
      "> 18\n",
      "> 19\n",
      "> 20\n",
      "> 21\n",
      "> 22\n",
      "> 23\n",
      "> 24\n",
      "> 25\n",
      "> 26\n",
      "> 27\n",
      "> 28\n",
      "> 29\n",
      "> 30\n",
      "> 31\n",
      "> 32\n",
      "> 33\n",
      "> 34\n",
      "> 35\n",
      "> 36\n",
      "> 37\n",
      "> 38\n",
      "> 39\n",
      "> 40\n",
      "> 41\n",
      "> 42\n",
      "> 43\n",
      "> 44\n",
      "> 45\n",
      "> 46\n",
      "> 47\n",
      "> 48\n",
      "> 49\n",
      "After new label added and before retraining (total: 7), accuracy of all nodes: 0.3157311669128508\n",
      "> 0\n",
      "> 1\n",
      "> 2\n",
      "> 3\n",
      "> 4\n",
      "> 5\n",
      "> 6\n",
      "> 7\n",
      "> 8\n",
      "> 9\n",
      "> 10\n",
      "> 11\n",
      "> 12\n",
      "> 13\n",
      "> 14\n",
      "> 15\n",
      "> 16\n",
      "> 17\n",
      "> 18\n",
      "> 19\n",
      "> 20\n",
      "> 21\n",
      "> 22\n",
      "> 23\n",
      "> 24\n",
      "> 25\n",
      "> 26\n",
      "> 27\n",
      "> 28\n",
      "> 29\n",
      "> 30\n",
      "> 31\n",
      "> 32\n",
      "> 33\n",
      "> 34\n",
      "> 35\n",
      "> 36\n",
      "> 37\n",
      "> 38\n",
      "> 39\n",
      "> 40\n",
      "> 41\n",
      "> 42\n",
      "> 43\n",
      "> 44\n",
      "> 45\n",
      "> 46\n",
      "> 47\n",
      "> 48\n",
      "> 49\n",
      "After new label added and after retraining (total: 7), accuracy of all nodes: 0.3050221565731167\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "#training \n",
    "#######################add label, assign vector to each node, knn, test and train (with predicted labels by knn)\n",
    "\n",
    "while len(possible_lab) > 0: #do it for every unseen label\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "    \n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    #assign vector\n",
    "    \n",
    "    #########\n",
    "    sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "    sub_tra_graph.copy_from_parent()\n",
    "    sub_tra_graph.register_message_func(m_func)\n",
    "    sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    ##labeled graph knn\n",
    "    out_nodes = th.LongTensor([])\n",
    "    for n in used_lab:\n",
    "        curr_used_lab = n\n",
    "        out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "    c=th.randperm(len(out_nodes)) #shuffle\n",
    "    out_nodes=out_nodes[c]\n",
    "\n",
    "    sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "    sub_labeled_graph.copy_from_parent()\n",
    "    sub_labeled_graph.register_message_func(m_func)\n",
    "    sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "    sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "    train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "    \n",
    "    #test\n",
    "    print(f\"After new label added and before retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "    #########\n",
    "    \n",
    "    addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer \n",
    "    for epoch in range(EPOCH):\n",
    "        print(\"> \" + str(epoch))\n",
    "        c=th.randperm(len(selected_nodes)) #shuffle\n",
    "        selected_nodes=selected_nodes[c]\n",
    "        epoch_nodes = selected_nodes[:EPOCH_SIZE] #select 20% of random nodes to train with at each epoch\n",
    "\n",
    "        error = []\n",
    "        for count in range(int(len(epoch_nodes)*.7), len(epoch_nodes)):\n",
    "            sub_graph = train_g.subgraph(epoch_nodes[:count])\n",
    "            sub_graph.copy_from_parent()\n",
    "            sub_graph.register_message_func(m_func)\n",
    "            sub_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "            feats = sub_graph.ndata['features']\n",
    "            labs = sub_graph.ndata['p_labels'] #true label\n",
    "\n",
    "            out = model(sub_graph, feats)\n",
    "            out2 = addon_layer(sub_graph, out) #extra layer\n",
    "            out2 = th.log_softmax(out2, 1)\n",
    "\n",
    "            loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "            error.append(loss.item())\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "\n",
    "#test\n",
    "print(f\"After new label added and after retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False,  True, False, False],\n",
      "        [False,  True, False, False,  True],\n",
      "        [ True, False,  True, False, False],\n",
      "        [False, False, False,  True, False],\n",
      "        [False,  True, False, False,  True]])\n",
      "tensor([[  0.0000,  12.4100,  19.6100,  83.6100,   9.6100],\n",
      "        [ 12.4100,   0.0000,   5.0000,  32.0000,  40.0000],\n",
      "        [ 19.6100,   5.0000,   0.0000,  29.0000,  41.0000],\n",
      "        [ 83.6100,  32.0000,  29.0000,   0.0000, 136.0000],\n",
      "        [  9.6100,  40.0000,  41.0000, 136.0000,   0.0000]])\n",
      "tensor([[ 0.0000,  0.0000, 19.6100,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, 40.0000],\n",
      "        [19.6100,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, 40.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[  0.0000,  12.4100,   0.0000,  83.6100,   9.6100],\n",
      "        [ 12.4100,   0.0000,   5.0000,  32.0000,   0.0000],\n",
      "        [  0.0000,   5.0000,   0.0000,  29.0000,  41.0000],\n",
      "        [ 83.6100,  32.0000,  29.0000,   0.0000, 136.0000],\n",
      "        [  9.6100,   0.0000,  41.0000, 136.0000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "j = th.LongTensor([2, 4, 2, 8, 4])\n",
    "p = th.FloatTensor([[1, 2.1],\n",
    "                  [3, 5],\n",
    "                  [5, 4],\n",
    "                  [7, 9],\n",
    "                  [1.0, -1.0]])\n",
    "\n",
    "m = same_label(j)\n",
    "n = similarity_matrix(p)\n",
    "print(m)\n",
    "print(n)\n",
    "print(m*n)\n",
    "print((m*-1 + 1)*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "total: 7, accuracy of all nodes: 0.34\n",
      "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9982,  1.0000, -0.9999,\n",
      "          0.9995, -1.0000],\n",
      "        [-0.9990,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9997,  1.0000, -0.9998,\n",
      "          0.9999, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.7579,  1.0000,  1.0000,  1.0000, -0.9950, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  0.9999],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9997, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9979],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9988,  1.0000,  1.0000,  0.9994, -0.9995,\n",
      "          1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9997,  1.0000,  0.9926,  1.0000, -0.9998,\n",
      "          0.9991, -1.0000],\n",
      "        [-0.9995,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9998,  1.0000,  0.9989,  1.0000, -0.9998,\n",
      "          0.9999, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9999, -0.9994,  1.0000, -1.0000,\n",
      "         -0.9067, -0.9999],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9715,  1.0000, -1.0000,\n",
      "          0.9980, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9980, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9919],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  0.9936,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9926,  1.0000,  1.0000,  1.0000, -0.9999, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9999,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 0.9999,  1.0000,  1.0000,  1.0000,  0.9959, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9759],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  0.9993,  1.0000, -0.9999,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9989, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9951],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9998],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9996,  1.0000,  0.9999,  0.9999, -0.9982,\n",
      "          1.0000, -1.0000],\n",
      "        [-0.9890,  1.0000,  1.0000,  1.0000, -0.9999, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 0.9999,  1.0000,  1.0000,  1.0000,  0.9929, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9442],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9999],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,  0.9999, -0.9995,\n",
      "          1.0000, -1.0000],\n",
      "        [-0.9998,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9999,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9998, -0.9999,  1.0000, -1.0000,\n",
      "         -0.9243, -0.9998],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9993,  1.0000,  0.9998,  0.9999, -0.9992,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9997,  1.0000,  1.0000,  0.9996, -0.9999,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9998,\n",
      "          1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  0.9998,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9792,  1.0000, -1.0000,\n",
      "          0.9978, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9875,  1.0000, -0.9999,\n",
      "          0.9962, -1.0000],\n",
      "        [-0.6074,  1.0000,  1.0000,  1.0000, -0.9925, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  0.9998],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9996,  1.0000,  1.0000,  0.9998, -0.9996,\n",
      "          1.0000, -1.0000],\n",
      "        [-0.7565,  1.0000,  1.0000,  1.0000, -0.9916, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  0.9999],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9996, -1.0000,  1.0000, -1.0000,\n",
      "         -0.9999, -0.9988],\n",
      "        [-0.9236,  1.0000,  1.0000,  1.0000, -0.9994, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  0.9999,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 0.7791,  1.0000,  1.0000,  1.0000, -0.9005, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  0.9987],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,  0.9999, -0.9998,\n",
      "          1.0000, -1.0000],\n",
      "        [-0.9935,  1.0000,  1.0000,  1.0000, -0.9999, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9998,  1.0000, -1.0000,\n",
      "         -0.9961, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -0.9994, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -0.9998, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9998,  1.0000, -1.0000,\n",
      "         -0.9802, -1.0000],\n",
      "        [-0.9794,  1.0000,  1.0000,  1.0000, -0.9996, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  0.9997,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,  1.0000, -0.9998,\n",
      "          1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  0.9973,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -0.9943, -1.0000],\n",
      "        [-0.9797,  1.0000,  1.0000,  1.0000, -0.9998, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9975,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9045,  1.0000,  1.0000,  1.0000, -0.9996, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9989,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 0.9998,  1.0000,  1.0000,  1.0000,  0.9959, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9766],\n",
      "        [-0.9987,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9997,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9993, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9959],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9997,  1.0000, -1.0000,\n",
      "         -0.9508, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -0.9998, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9997,  1.0000,  0.9998,  0.9999, -0.9997,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,  1.0000, -0.9990,\n",
      "          1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  0.9998,  1.0000, -0.9998,\n",
      "          1.0000, -1.0000],\n",
      "        [-0.9990,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9954, -0.9999,  1.0000, -1.0000,\n",
      "         -0.8048, -0.9887],\n",
      "        [-0.9738,  1.0000,  1.0000,  1.0000, -0.9997, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9982,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9009,  1.0000,  1.0000,  1.0000, -0.9985, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9998,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000,  0.9999,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9975],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9996, -1.0000,\n",
      "          1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -0.9999,  1.0000, -1.0000,\n",
      "         -0.9392, -1.0000],\n",
      "        [ 0.9998,  1.0000,  1.0000,  1.0000,  0.9949, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -0.9651],\n",
      "        [-1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9991,  1.0000,  0.9999,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9995,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,  0.9999, -0.9996,\n",
      "          1.0000, -1.0000],\n",
      "        [-1.0000,  1.0000,  0.9998,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9999,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.9961,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000],\n",
      "        [-0.7229,  1.0000,  1.0000,  1.0000, -0.9993, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000]], device='cuda:0', grad_fn=<TanhBackward>)\n",
      "tensor([1, 3, 4, 0, 2, 2, 1, 5, 1, 5, 1, 1, 4, 0, 2, 5, 6, 3, 3, 2, 2, 1, 0, 2,\n",
      "        1, 2, 0, 2, 0, 0, 2, 5, 2, 0, 3, 5, 4, 1, 1, 1, 6, 4, 2, 2, 1, 3, 0, 2,\n",
      "        6, 3, 3, 4, 5, 2, 2, 2, 6, 3, 2, 3, 0, 0, 2, 6, 2, 6, 3, 4, 5, 2, 2, 1,\n",
      "        2, 5, 1, 2, 3, 1, 1, 6, 4, 3, 6, 3, 5, 3, 0, 3, 6, 2, 1, 6, 6, 6, 6, 1,\n",
      "        5, 3, 6, 5], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_look = 100\n",
    "\n",
    "sub_graph1 = train_g.subgraph(selected_nodes[:num_look])\n",
    "sub_graph1.copy_from_parent()\n",
    "sub_graph1.register_message_func(m_func)\n",
    "sub_graph1.register_reduce_func(m_reduce_func)\n",
    "\n",
    "sub_graph2 = labeled_g.subgraph(out_nodes)\n",
    "sub_graph2.copy_from_parent()\n",
    "sub_graph2.register_message_func(m_func)\n",
    "sub_graph2.register_reduce_func(m_reduce_func)\n",
    "\n",
    "print(sub_graph2.ndata['features'])\n",
    "\n",
    "knn.fit(model(sub_graph2, sub_graph2.ndata['features']).cpu().detach().numpy(), sub_graph2.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_graph1.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_graph1, sub_graph1.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print(f\"total: {len(used_lab)}, accuracy of all nodes: {metrics.accuracy_score((sub_graph1.ndata['t_labels']).cpu().detach().numpy(), (sub_graph1.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n",
    "feats = sub_graph1.ndata['features']\n",
    "labs = sub_graph1.ndata['t_labels'] #true label\n",
    "output = model(sub_graph1, feats)\n",
    "print(output)\n",
    "print(labs)\n",
    "\n",
    "\n",
    "pos = defaultdict(list)\n",
    "cos = []\n",
    "cos2 = []\n",
    "\n",
    "for n in range(num_look):\n",
    "    pos[n] = (output[n][0].item(), output[n][1].item())\n",
    "    cos.append(sub_graph1.ndata['p_labels'][n].item())\n",
    "    cos2.append(sub_graph1.ndata['t_labels'][n].item())\n",
    "\n",
    "nx.draw_networkx_nodes(sub_graph1.to_networkx(), pos, node_color=cos2, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=150)\n",
    "nx.draw(sub_graph1.to_networkx(), pos, node_color=cos, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.any(th.isnan(sub_labeled_graph.ndata['features']))\n",
    "th.all(th.isfinite(sub_labeled_graph.ndata['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
