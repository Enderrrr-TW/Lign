{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGN\n",
    "\n",
    "Graph Induced Lifelong Learning through Features Similarities and Dissimilarities \n",
    "\n",
    "----\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lign as lg\n",
    "import lign.models as md\n",
    "import lign.utils as utl\n",
    "\n",
    "import torch as th\n",
    "import torchvision as tv\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "tm_now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Preprocessing \n",
    "\n",
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"CIFAR\" #<<<<<\n",
    "\n",
    "trans = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = utl.load.cifar_to_lign(\"data/datasets/CIFAR100\", transforms = trans)\n",
    "dataset.save(\"data/datasets/cifar100_train.lign\")\n",
    "\n",
    "validate = utl.load.cifar_to_lign(\"data/datasets/CIFAR100\", train=False, transforms = trans)\n",
    "validate.save(\"data/datasets/cifar100_test.lign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"MNIST\" #<<<<<\n",
    "\n",
    "trans = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = utl.load.mnist_to_lign(\"data/datasets/MNIST\", transforms = trans)\n",
    "dataset.save(\"data/datasets/mnist_train.lign\")\n",
    "\n",
    "validate = utl.load.mnist_to_lign(\"data/datasets/MNIST\", train=False, transforms = trans)\n",
    "validate.save(\"data/datasets/mnist_test.lign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"CIFAR\" #<<<<<\n",
    "\n",
    "dataset = lg.graph.GraphDataset(\"data/datasets/cifar100_train.lign\")\n",
    "validate = lg.graph.GraphDataset(\"data/datasets/cifar100_test.lign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"MNIST\" #<<<<<\n",
    "\n",
    "dataset = lg.graph.GraphDataset(\"data/datasets/mnist_train.lign\")\n",
    "validate = lg.graph.GraphDataset(\"data/datasets/mnist_test.lign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if th.cuda.is_available():\n",
    "    device = th.device(\"cuda\")\n",
    "    th.cuda.empty_cache()\n",
    "else:\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_neighs_data(neighs): ## adds up neighbors' data before executing post_mod (pre_mod happens before)\n",
    "    out = neighs[0]\n",
    "    for neigh in neighs[1:]:\n",
    "        out = out + neigh\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "* LAMBDA: rate by which EPOCHS decreases over retraining periods\n",
    "* DIST_VEC_SIZE: size of vector representing the mapping of the nodes by the model\n",
    "* INIT_NUM_LAB: number of labels used to training the model initially in the supervised method to learn pairwise mapping\n",
    "* LABELS: list of all the labels that model comes across. Labels can be appended at any time. The order of labels is initially randomized\n",
    "* SUBGRAPH_SIZE: represent the number of nodes processed at once. The models don't have batches. This is the closest thing to it\n",
    "* AMP_ENABLE: toggle to enable mixed precission training\n",
    "* EPOCHS: Loops executed during training\n",
    "* LR: Learning rate\n",
    "* RETRAIN_PER: period between retraining based on number of labels seen. format: (offset, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 0.05\n",
    "DIST_VEC_SIZE = 200 #128\n",
    "INIT_NUM_LAB = 50\n",
    "LABELS = np.arange(100)\n",
    "SUBGRPAH_SIZE = 500\n",
    "AMP_ENABLE = True\n",
    "EPOCHS = 300\n",
    "LR = 1e-3\n",
    "RETRAIN_PER = {\n",
    "    \"superv\": (6, 3), # (offset, frequency)\n",
    "    #\"semi\": (0, 15)\n",
    "}\n",
    "\n",
    "np.random.shuffle(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models\n",
    "### LIGN\n",
    "\n",
    "[L]ifelong Learning [I]nduced by [G]raph [N]eural Networks Model (LIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIFAR\n",
    "class LIGN_CIFAR(nn.Module):\n",
    "    def __init__(self, out_feats):\n",
    "        super(LIGN_CIFAR, self).__init__()\n",
    "        self.gcn1 = lg.layers.GCN(nn.Conv2d(1, 32, 3, 1))\n",
    "        self.gcn2 = lg.layers.GCN(nn.Conv2d(32, 64, 3, 1))\n",
    "        self.gcn3 = lg.layers.GCN(nn.Linear(9216, out_feats))\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, g, features, save=False):\n",
    "        x = F.relu(self.gcn1(g, features))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            print(x_sv.shape)\n",
    "            for i in range(x_sv.size(0)):\n",
    "                save_image(th.unsqueeze(x_sv[i], 1), \"data/thesis/mid/gcn1/img\" + str(i) + \".png\")\n",
    "\n",
    "        x = F.relu(self.gcn2(g, x))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            for i in range(x_sv.size(0)):\n",
    "                save_image(th.unsqueeze(x_sv[i], 1), \"data/thesis/mid/gcn2/img\" + str(i) + \".png\")\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = th.flatten(self.drop1(x), 1)\n",
    "        x = F.relu(self.gcn3(g, x))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            save_image(th.unsqueeze(th.unsqueeze(x_sv, 1), -1), \"data/thesis/mid/vector output.png\", nrow=100)\n",
    "\n",
    "        return self.drop2(x)\n",
    "\n",
    "base = LIGN_CIFAR(DIST_VEC_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST\n",
    "class LIGN_MNIST(nn.Module):\n",
    "    def __init__(self, out_feats):\n",
    "        super(LIGN_MNIST, self).__init__()\n",
    "        self.gcn1 = lg.layers.GCN(nn.Conv2d(1, 32, 3, 1))\n",
    "        self.gcn2 = lg.layers.GCN(nn.Conv2d(32, 64, 3, 1))\n",
    "        self.gcn3 = lg.layers.GCN(nn.Linear(9216, out_feats))\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, g, features, save=False):\n",
    "        x = F.relu(self.gcn1(g, features))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            print(x_sv.shape)\n",
    "            for i in range(x_sv.size(0)):\n",
    "                save_image(th.unsqueeze(x_sv[i], 1), \"data/thesis/mid/gcn1/img\" + str(i) + \".png\")\n",
    "\n",
    "        x = F.relu(self.gcn2(g, x))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            for i in range(x_sv.size(0)):\n",
    "                save_image(th.unsqueeze(x_sv[i], 1), \"data/thesis/mid/gcn2/img\" + str(i) + \".png\")\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = th.flatten(self.drop1(x), 1)\n",
    "        x = F.relu(self.gcn3(g, x))\n",
    "        if(save):\n",
    "            x_sv = x[:100]\n",
    "            save_image(th.unsqueeze(th.unsqueeze(x_sv, 1), -1), \"data/thesis/mid/vector output.png\", nrow=100)\n",
    "\n",
    "        return self.drop2(x)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module): ## temporality layer for training\n",
    "    def __init__(self, in_fea, out_fea, device = 'cuda'):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.addon = lg.layers.ADDON(in_fea, out_fea, device=device) # dynamic linear dense layer\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = F.log_softmax(self.addon(g, features), dim=1)\n",
    "        return x\n",
    "\n",
    "base = LIGN_MNIST(DIST_VEC_SIZE).to(device) # base\n",
    "classifier = Classifier(DIST_VEC_SIZE, INIT_NUM_LAB, device).to(device) # classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Training\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt\n",
    "accuracy = []\n",
    "log = []\n",
    "num_of_labels = len(LABELS)\n",
    "\n",
    "opt = th.optim.Adam([ # optimizer for the full\n",
    "        {'params': base.parameters()},\n",
    "        {'params': classifier.parameters(), 'lr': 1e-3}\n",
    "    ], lr=1e-2)\n",
    "\n",
    "scaler = GradScaler() if AMP_ENABLE else None\n",
    "\n",
    "retrain_superv = lambda x: (x + RETRAIN_PER[\"superv\"][0])%RETRAIN_PER[\"superv\"][1] == 0\n",
    "#retrain_semi = lambda x: (x + RETRAIN_PER[\"semi\"][0])%RETRAIN_PER[\"semi\"][1] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load('data/models/LIGN_training_cool_time.pt')\n",
    "\n",
    "base.load_state_dict(checkpoint['base'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "opt.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "if AMP_ENABLE:\n",
    "    scaler.load_state_dict(checkpoint['scaler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4/10 -- Accuracy: 86.07% -- Original\n",
      "Label: 4/10 -- Accuracy: 98.86% -- Semisurpervised Retraining: False -- Surpervised Retraining: False\n",
      "Label: 5/10 -- Accuracy: 92.02% -- Semisurpervised Retraining: False -- Surpervised Retraining: False\n",
      "Label: 6/10 -- Accuracy: 97.91% -- Semisurpervised Retraining: False -- Surpervised Retraining: True\n",
      "Label: 7/10 -- Accuracy: 91.1% -- Semisurpervised Retraining: False -- Surpervised Retraining: False\n",
      "Label: 8/10 -- Accuracy: 83.92% -- Semisurpervised Retraining: False -- Surpervised Retraining: False\n",
      "Label: 9/10 -- Accuracy: 95.35% -- Semisurpervised Retraining: False -- Surpervised Retraining: True\n",
      "Label: 10/10 -- Accuracy: 93.88% -- Semisurpervised Retraining: False -- Surpervised Retraining: False\n"
     ]
    }
   ],
   "source": [
    "def test_and_save(num_labels, text, method=\"NN\"):\n",
    "    acc = lg.test.accuracy(base, validate, dataset, \"x\", \"labels\", LABELS[:num_labels], cluster=(utl.clustering.NN(), 5), device=device)\n",
    "    accuracy.append(acc)\n",
    "    log.append(\"Label: {}/{} -- Accuracy({}): {}% -- {}\".format(num_labels, num_of_labels, method, round(acc, 2), text))\n",
    "    print(log[-1])\n",
    "\n",
    "# original network\n",
    "test_and_save(INIT_NUM_LAB, \"Original\", method=\"NN\")\n",
    "\n",
    "lg.train.superv(base, classifier, opt, dataset, \"x\", \"labels\", LABELS[:INIT_NUM_LAB], (device, scaler), epochs=EPOCHS, subgraph_size=SUBGRPAH_SIZE)\n",
    "\n",
    "# trained network\n",
    "test_and_save(INIT_NUM_LAB, \"Initial Training\", method=\"NN\")\n",
    "\n",
    "# online learning system\n",
    "for num_labels in range(INIT_NUM_LAB+1, num_of_labels+1):\n",
    "\n",
    "    if retrain_superv(num_labels):\n",
    "        test_and_save(num_labels, \"Before Supervised Retraining\", method=\"NN\")\n",
    "\n",
    "        addon.addon.update_size(num_labels)\n",
    "        EPOCHS -= int(EPOCHS*LAMBDA)\n",
    "        lg.train.superv(base, classifier, opt, dataset, \"x\", \"labels\", LABELS[:num_labels], (device, scaler), epochs=EPOCHS, subgraph_size=SUBGRPAH_SIZE)\n",
    "    \n",
    "    test_and_save(num_labels, \"Test with \" + str(LABELS[num_labels]), method=\"NN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = str(tm_now()).replace(\":\", \"-\").replace(\".\", \"\").replace(\" \", \"_\")\n",
    "filename = \"LIGN_\" + dataset_name + \"_training_\"+time\n",
    "\n",
    "## Save metrics\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"log\": log\n",
    "}\n",
    "utl.io.json(metrics, \"data/metrics/\"+filename+\".json\")\n",
    "\n",
    "## Save hyperparameters\n",
    "para = {\n",
    "    \"LAMBDA\": LAMBDA,\n",
    "    \"DIST_VEC_SIZE\": DIST_VEC_SIZE,\n",
    "    \"INIT_NUM_LAB\": INIT_NUM_LAB,\n",
    "    \"LABELS\": LABELS.tolist(),\n",
    "    \"SUBGRPAH_SIZE\": SUBGRPAH_SIZE,\n",
    "    \"AMP_ENABLE\": AMP_ENABLE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"RETRAIN_PER\": RETRAIN_PER,\n",
    "    \"STRUCTURE\": {\n",
    "        \"base\": str(base),\n",
    "        \"classifier\": str(classifier)\n",
    "    }\n",
    "}\n",
    "\n",
    "utl.io.json(para, \"data/parameters/\"+filename+\".json\")\n",
    "\n",
    "## Save model\n",
    "check = {\n",
    "    \"base\": base.state_dict(),\n",
    "    \"classifier\": base.state_dict(),\n",
    "    \"optimizer\": opt.state_dict()\n",
    "}\n",
    "if AMP_ENABLE:\n",
    "    check[\"scaler\"] = scaler.state_dict()\n",
    "\n",
    "th.save(check, \"data/models/\"+filename+\".pt\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}