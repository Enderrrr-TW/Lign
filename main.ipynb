{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGN\n",
    "Graph Induced Lifelong Learning for Spatial-Temporal Data\n",
    "\n",
    "----\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lign as lg\n",
    "import lign.models as md\n",
    "import lign.utils as utl\n",
    "\n",
    "import torch as th\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "tm_now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Preprocessing \n",
    "\n",
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = utl.io.cifar_to_lign(\"data/datasets/CIFAR100\", transforms = trans)\n",
    "dataset.save(\"data/datasets/cifar100_train.lign\")\n",
    "\n",
    "validate = utl.io.cifar_to_lign(\"data/datasets/CIFAR100\", train=False, transforms = trans)\n",
    "validate.save(\"data/datasets/cifar100_test.lign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lg.graph.GraphDataset(\"data/datasets/cifar100_train.lign\")\n",
    "validate = lg.graph.GraphDataset(\"data/datasets/cifar100_test.lign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if th.cuda.is_available():\n",
    "    device = th.device(\"cuda\")\n",
    "    th.cuda.empty_cache()\n",
    "else:\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_neighs_data(neighs): ## adds up neighbors' data before executing post_mod (pre_mod happens before)\n",
    "    out = neighs[0]\n",
    "    for neigh in neighs[1:]:\n",
    "        out = out + neigh\n",
    "    return out\n",
    "\n",
    "class ADDON(nn.Module): ## tempory layer for training\n",
    "    def __init__(self, in_fea, out_fea):\n",
    "        super(ADDON, self).__init__()\n",
    "        self.gcn1 = md.layers.GCN(post_mod = nn.Linear(in_fea, in_fea * 6))\n",
    "        self.gcn2 = md.layers.GCN(nn.Linear(in_fea * 6, out_fea))\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        return self.gcn2(g, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "* LAMBDA: regulates how much the model relies on difference between the nodes vs the features that lead to their label when calculating pairwise loss\n",
    "* DIST_VEC_SIZE: size of vector representing the mapping of the nodes by the model\n",
    "* INIT_NUM_LAB: number of labels used to training the model initially in the supervised method to learn pairwise mapping\n",
    "* LABELS: list of all the labels that model comes across. Labels can be appended at any time. The order of labels is initially randomized\n",
    "* SUBGRAPH_SIZE: represent the number of nodes processed at once. The models don't have batches. This is the closest thing to it\n",
    "* AMP_ENABLE: toggle to enable mixed precission training\n",
    "* EPOCHS: Loops executed during training\n",
    "* LR: Learning rate\n",
    "* RETRAIN_PER: period between retraining based on number of labels seen. format: (offset, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 25\n",
    "DIST_VEC_SIZE = 2 # 3 was picked so the graph can be drawn in a 3d grid\n",
    "INIT_NUM_LAB = 20\n",
    "LABELS = np.arange(40)\n",
    "SUBGRPAH_SIZE = 700\n",
    "AMP_ENABLE = True\n",
    "EPOCHS = 1200\n",
    "LR = 1e-3\n",
    "RETRAIN_PER = {\n",
    "    \"superv\": (7, 10),\n",
    "    \"semi\": (0, 15)\n",
    "}\n",
    "\n",
    "np.random.shuffle(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models\n",
    "### LIGN\n",
    "\n",
    "[L]ifelong Learning [I]nduced by [G]raph [N]eural Networks Model (LIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIGN_CIFAR(nn.Module):\n",
    "    def __init__(self, out_feats):\n",
    "        super(LIGN_CIFAR, self).__init__()\n",
    "        self.gcn1 = md.layers.GCN(nn.Conv2d(3, 6, 5))\n",
    "        self.gcn2 = md.layers.GCN(nn.Conv2d(6, 16, 5))\n",
    "        self.gcn3 = md.layers.GCN(nn.Linear(16 * 5 * 5, 150))\n",
    "        self.gcn4 = md.layers.GCN(nn.Linear(150, 84))\n",
    "        self.gcn5 = md.layers.GCN(nn.Linear(84, out_feats))\n",
    "        self.pool = md.layers.GCN(nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.pool(g, F.relu(self.gcn1(g, features)))\n",
    "        x = self.pool(g, F.relu(self.gcn2(g, x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.gcn3(g, x))\n",
    "        x = F.relu(self.gcn4(g, x))\n",
    "        \n",
    "        return th.tanh(self.gcn5(g, x))\n",
    "\n",
    "model = LIGN_CIFAR(DIST_VEC_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-LIGN\n",
    "[R]ecurrent [L]ifelong Learning [I]nduced by [G]raph [N]eural Networks Model (R-LIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.set_data(\"h\", )\n",
    "#dataset.set_data(\"c\", )\n",
    "####\n",
    "# model = R_LIGN(DIST_VEC_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Training\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt\n",
    "accuracy = []\n",
    "log = []\n",
    "num_of_labels = len(LABELS)\n",
    "opt = th.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = GradScaler() if AMP_ENABLE else None\n",
    "\n",
    "retrain_superv = lambda x: x%RETRAIN_PER[\"superv\"][1] == RETRAIN_PER[\"superv\"][0]\n",
    "retrain_semi = lambda x: x%RETRAIN_PER[\"semi\"][1] == RETRAIN_PER[\"semi\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/models/LIGN_training_cool_time.pt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-157f682b67a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/models/LIGN_training_cool_time.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LIGN\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LIGN\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LIGN\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/models/LIGN_training_cool_time.pt'"
     ]
    }
   ],
   "source": [
    "checkpoint = th.load('data/models/LIGN_training_cool_time.pt')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "opt.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "if AMP_ENABLE:\n",
    "    scaler.load_state_dict(checkpoint['scaler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Label: 20/40\t|\tAccuracy: 20.0\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 21/40\t|\tAccuracy: 18.57\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 22/40\t|\tAccuracy: 17.36\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 23/40\t|\tAccuracy: 16.0\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 24/40\t|\tAccuracy: 15.17\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 25/40\t|\tAccuracy: 14.0\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 26/40\t|\tAccuracy: 13.19\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 27/40\t|\tAccuracy: 15.89\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: True\nLabel: 28/40\t|\tAccuracy: 15.21\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 29/40\t|\tAccuracy: 14.28\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 30/40\t|\tAccuracy: 13.67\t|\tSemisurpervised Retraining: True\t|\tSurpervised Retraining: False\nLabel: 31/40\t|\tAccuracy: 13.16\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 32/40\t|\tAccuracy: 12.72\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 33/40\t|\tAccuracy: 12.18\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 34/40\t|\tAccuracy: 11.35\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 35/40\t|\tAccuracy: 11.2\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 36/40\t|\tAccuracy: 10.61\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 37/40\t|\tAccuracy: 8.92\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: True\nLabel: 38/40\t|\tAccuracy: 8.58\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 39/40\t|\tAccuracy: 8.33\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\nLabel: 40/40\t|\tAccuracy: 8.07\t|\tSemisurpervised Retraining: False\t|\tSurpervised Retraining: False\n"
    }
   ],
   "source": [
    "lg.train.superv(model, opt, dataset, \"x\", \"labels\", DIST_VEC_SIZE, LABELS[:INIT_NUM_LAB], LAMBDA, (device, scaler), addon = ADDON, subgraph_size=SUBGRPAH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "for num_labels in range(INIT_NUM_LAB, num_of_labels + 1):\n",
    "\n",
    "    \"\"\"if retrain_semi(num_labels):\n",
    "        lg.train.semi_superv(model, opt, dataset, \"x\", \"labels\", DIST_VEC_SIZE, LABELS[:num_labels], LAMBDA, (device, scaler), addon = ADDON, subgraph_size=SUBGRPAH_SIZE, epochs=EPOCHS, cluster=(utl.clustering.NN(), 5))\"\"\"\n",
    "\n",
    "    if retrain_superv(num_labels):\n",
    "        lg.train.superv(model, opt, dataset, \"x\", \"labels\", DIST_VEC_SIZE, LABELS[:num_labels], LAMBDA, (device, scaler), epochs=EPOCHS, addon = ADDON, subgraph_size=SUBGRPAH_SIZE)\n",
    "    \n",
    "    acc = lg.test.accuracy(model, validate, dataset, \"x\", \"labels\", LABELS[:num_labels], cluster=(utl.clustering.NN(), 5), sv_img = '2d', device=device)\n",
    "\n",
    "    accuracy.append(acc)\n",
    "    log.append(\"Label: {}/{}\\t|\\tAccuracy: {}\\t|\\tSemisurpervised Retraining: {}\\t|\\tSurpervised Retraining: {}\".format(num_labels, num_of_labels, round(acc, 2), retrain_semi(num_labels), retrain_superv(num_labels)))\n",
    "    print(log[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = str(tm_now()).replace(\":\", \"-\").replace(\".\", \"\").replace(\" \", \"_\")\n",
    "filename = \"LIGN_CIFAR_training_\"+time\n",
    "\n",
    "## Save metrics\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"log\": log\n",
    "}\n",
    "utl.io.json(metrics, \"data/metrics/\"+filename+\".json\")\n",
    "\n",
    "## Save hyperparameters\n",
    "para = {\n",
    "    \"LAMBDA\": LAMBDA,\n",
    "    \"DIST_VEC_SIZE\": DIST_VEC_SIZE,\n",
    "    \"INIT_NUM_LAB\": INIT_NUM_LAB,\n",
    "    \"LABELS\": LABELS.tolist(),\n",
    "    \"SUBGRPAH_SIZE\": SUBGRPAH_SIZE,\n",
    "    \"AMP_ENABLE\": AMP_ENABLE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"RETRAIN_PER\": RETRAIN_PER\n",
    "}\n",
    "\n",
    "utl.io.json(para, \"data/parameters/\"+filename+\".json\")\n",
    "\n",
    "LAMBDA = 20\n",
    "DIST_VEC_SIZE = 2 # 3 was picked so the graph can be drawn in a 3d grid\n",
    "INIT_NUM_LAB = 20\n",
    "LABELS = np.arange(30)\n",
    "SUBGRPAH_SIZE = 500\n",
    "AMP_ENABLE = True\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "\n",
    "## Save model\n",
    "check = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": opt.state_dict()\n",
    "}\n",
    "if AMP_ENABLE:\n",
    "    check[\"scaler\"] = scaler.state_dict()\n",
    "\n",
    "th.save(check, \"data/models/\"+filename+\".pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## View\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    tr_nodes, tr_labs = utl.clustering.filter_k('labels', LABELS, dataset, 5)\n",
    "    sub = dataset.subgraph(tr_nodes)\n",
    "    inp = sub.get_parent_data('x').to(device)\n",
    "\n",
    "\n",
    "    cluster = utl.clustering.NN()\n",
    "    cluster.train(model(sub, inp), tr_labs.to(device))\n",
    "\n",
    "    inp = validate.get_data('x').to(device)\n",
    "    outp_t = validate.get_data('labels').to(device)\n",
    "\n",
    "    rep_vec = model(validate, inp)\n",
    "    outp_p = cluster(rep_vec)\n",
    "    print(model(sub, inp))\n",
    "\n",
    "print(outp_p[:15])\n",
    "print(outp_t[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}