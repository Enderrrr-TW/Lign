{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Libraries ############\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### GPU Setup ############\n",
    "\n",
    "#set gpu if available\n",
    "if th.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    #device = th.device(\"cuda\")\n",
    "    device = th.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model Definition ############\n",
    "\n",
    "#operation for neigbors\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "    \n",
    "#gcn layer in network\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.pull(g.nodes())\n",
    "        g.apply_nodes(self.apply_mod)\n",
    "        \n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "#network\n",
    "class LIGN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(LIGN, self).__init__()\n",
    "        self.gcn1 = GCN(in_feats, 100, F.relu)\n",
    "        self.gcn2 = GCN(100, 30, F.relu)\n",
    "        self.gcn3 = GCN(30, out_feats, th.tanh)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        \n",
    "        return self.gcn3(g, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Loss Functions ############\n",
    "\n",
    "def similarity_matrix(x): #pairwise distance\n",
    "    \"\"\"x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y = x\n",
    "    y_norm = x_norm.view(1, -1)\n",
    "\n",
    "    dist = x_norm + y_norm - 2.0 * th.mm(x, th.transpose(y, 0, 1))\"\"\"\n",
    "    n = x.size(0)\n",
    "    d = x.size(1)\n",
    "\n",
    "    y = x.unsqueeze(0).expand(n, n, d)\n",
    "    x = x.unsqueeze(1).expand(n, n, d)\n",
    "    \n",
    "    dist = th.pow(x - y, 2).sum(2)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def same_label(y):\n",
    "    s = y.size(0)\n",
    "    y_expand = y.unsqueeze(0).expand(s, s)\n",
    "    Y = y_expand.eq(y_expand.t())\n",
    "    return Y\n",
    "\n",
    "def pairwaise_loss(output, labels):\n",
    "    \"\"\"\n",
    "    if nodes with the same label: x^2\n",
    "    if nodes with different label: -x^2\n",
    "    \"\"\"\n",
    "    sim = similarity_matrix(output)\n",
    "    same = same_label(labels)\n",
    "    diff = temp*(-1) + 1  #turns 1's to 0's and 0's to 1's \n",
    "    \n",
    "    same_M = (same * sim)\n",
    "    diff_M = (diff * sim)\n",
    "    loss = (th.sum(same_M)/th.sum(same))**2 - (th.sum(diff_M)/th.sum(diff))**2\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Load Dataset ############\n",
    "glist, _ = load_graphs(\"mnist/graph.bin\")\n",
    "\n",
    "ds_g = glist[0]\n",
    "\n",
    "ds_g.ndata['features'] = th.FloatTensor(ds_g.ndata['features']).to(device)\n",
    "ds_g.ndata['t_labels'] = th.LongTensor(ds_g.ndata['t_labels']).to(device)\n",
    "\n",
    "# to coordinate sending of features over the graph network\n",
    "m_func = fn.copy_src(src='h', out='m')\n",
    "m_reduce_func = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########### Hyper-parameters & Model ############\n",
    "\n",
    "# hyper parameters\n",
    "DIST_VEC_SIZE = 10\n",
    "NUMBER_OF_LABELS = th.unique(ds_g.ndata[\"t_labels\"]).size(0)\n",
    "k_NEIGHBOR = 3\n",
    "EPOCH = 50\n",
    "INIT_NUM_LAB = 3 #must be at least 1\n",
    "LABEL_NODES = k_NEIGHBOR+10\n",
    "Lambda = 0.0001\n",
    "\n",
    "model = LIGN(ds_features.size()[1], DIST_VEC_SIZE).to(device)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=k_NEIGHBOR)\n",
    "opt = th.optim.Adam(model.parameters(), lr=1e-3)# only run once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Supervised Learning ############\n",
    "\n",
    "train_g = ds_g.subgraph(ds_g.nodes()[:int(len(ds_g) * 1)])#100 percent of all the nodes\n",
    "train_g.copy_from_parent()\n",
    "\n",
    "labeled_nodes = th.LongTensor([])\n",
    "for i in range(NUMBER_OF_LABELS):\n",
    "    def has_feature_one(nodes): return (nodes.data['t_labels'] == i).squeeze(0)\n",
    "    \n",
    "    filt = train_g.filter_nodes(has_feature_one)\n",
    "    labeled_nodes = th.cat((filt[:k_NEIGHBOR+10], labeled_nodes), -1)\n",
    "    \n",
    "\n",
    "labeled_g = train_g.subgraph(labeled_nodes) #labels nodes in \n",
    "labeled_g.copy_from_parent()\n",
    "\n",
    "\"\"\"#both dataset should include all labels\n",
    "while th.unique(train_g.ndata[\"t_labels\"]).size(0) != th.unique(labeled_g.ndata[\"t_labels\"]).size(0):\n",
    "    c=th.randperm(len(ds_g.nodes())) #shuffle\n",
    "    ds_g.nodes =ds_g.nodes()[c]\n",
    "    \n",
    "    train_g = ds_g.subgraph(ds_g.nodes()[:int(len(ds_g) * 1)])#100 percent of all the nodes\n",
    "    labeled_g = ds_g.subgraph(ds_g.nodes()[int(len(ds_g) * .20):]) #20 percent labeled nodes for knn\n",
    "    train_g.copy_from_parent()\n",
    "    labeled_g.copy_from_parent()\"\"\"\n",
    "\n",
    "\n",
    "possible_lab = list(range(NUMBER_OF_LABELS))\n",
    "rand.shuffle(possible_lab) #set random order for how labels are added\n",
    "\n",
    "used_lab = []\n",
    "curr_used_lab = 0\n",
    "\n",
    "EPOCH_SIZE = 0\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "#############train with 2 labels of 7 labels (uses true label from the dataset), assign vector to each node,\n",
    "            #knn (1 labeled node for each label), test\n",
    "selected_nodes = train_g.filter_nodes(has_1_rand_label) #add two unseen labels\n",
    "used_lab.append(possible_lab.pop(0))\n",
    "for i in range(INIT_NUM_LAB - 1):\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "\n",
    "c=th.randperm(len(selected_nodes)) #shuffle\n",
    "selected_nodes=selected_nodes[c]\n",
    "\n",
    "EPOCH_SIZE = int(len(selected_nodes)*.80)\n",
    "\n",
    "#train\n",
    "addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer \n",
    "for epoch in range(EPOCH):\n",
    "    print(\"> \" + str(epoch))\n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    epoch_nodes = selected_nodes[:EPOCH_SIZE] #selected 20% of random nodes to train with at each epoch\n",
    "    \n",
    "    error = []\n",
    "    for count in range(int(len(epoch_nodes)*.7), len(epoch_nodes)):\n",
    "        sub_graph = train_g.subgraph(epoch_nodes[:count])\n",
    "        sub_graph.copy_from_parent()\n",
    "        sub_graph.register_message_func(m_func)\n",
    "        sub_graph.register_reduce_func(m_reduce_func)\n",
    "        \n",
    "        feats = sub_graph.ndata['features']\n",
    "        labs = sub_graph.ndata['t_labels'] #true label\n",
    "        \n",
    "        out = model(sub_graph, feats)\n",
    "        \n",
    "        out2 = addon_layer(sub_graph, out) #extra layer\n",
    "        out2 = th.log_softmax(out2, 1)\n",
    "        \n",
    "        loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "        error.append(loss.item())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    #print(np.mean(error))\n",
    "\n",
    "    \n",
    "#knn and assign vector\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy())\n",
    "print((sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())\n",
    "\n",
    "#test\n",
    "print(f\"> Initial training with {INIT_NUM_LAB} labels (distinction efficiency): {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Unsupervised Learning ############\n",
    "\n",
    "while len(possible_lab) > 0: #do it for every unseen label\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "    \n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    #assign vector\n",
    "    \n",
    "    #########\n",
    "    sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "    sub_tra_graph.copy_from_parent()\n",
    "    sub_tra_graph.register_message_func(m_func)\n",
    "    sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    ##labeled graph knn\n",
    "    out_nodes = th.LongTensor([])\n",
    "    for n in used_lab:\n",
    "        curr_used_lab = n\n",
    "        out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "    c=th.randperm(len(out_nodes)) #shuffle\n",
    "    out_nodes=out_nodes[c]\n",
    "\n",
    "    sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "    sub_labeled_graph.copy_from_parent()\n",
    "    sub_labeled_graph.register_message_func(m_func)\n",
    "    sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "    sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "    train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "    \n",
    "    #test\n",
    "    print(f\"After new label added and before retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "    #########\n",
    "    \n",
    "    addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer \n",
    "    for epoch in range(EPOCH):\n",
    "        print(\"> \" + str(epoch))\n",
    "        c=th.randperm(len(selected_nodes)) #shuffle\n",
    "        selected_nodes=selected_nodes[c]\n",
    "        epoch_nodes = selected_nodes[:EPOCH_SIZE] #select 20% of random nodes to train with at each epoch\n",
    "\n",
    "        error = []\n",
    "        for count in range(int(len(epoch_nodes)*.7), len(epoch_nodes)):\n",
    "            sub_graph = train_g.subgraph(epoch_nodes[:count])\n",
    "            sub_graph.copy_from_parent()\n",
    "            sub_graph.register_message_func(m_func)\n",
    "            sub_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "            feats = sub_graph.ndata['features']\n",
    "            labs = sub_graph.ndata['p_labels'] #true label\n",
    "\n",
    "            out = model(sub_graph, feats)\n",
    "            out2 = addon_layer(sub_graph, out) #extra layer\n",
    "            out2 = th.log_softmax(out2, 1)\n",
    "\n",
    "            loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "            error.append(loss.item())\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "\n",
    "#test\n",
    "print(f\"After new label added and after retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = th.LongTensor([2, 4, 2, 8, 4])\n",
    "p = th.FloatTensor([[1, 2.1],\n",
    "                  [3, 5],\n",
    "                  [5, 4],\n",
    "                  [7, 9],\n",
    "                  [1.0, -1.0]])\n",
    "\n",
    "m = same_label(j)\n",
    "n = similarity_matrix(p)\n",
    "print(m)\n",
    "print(n)\n",
    "print(m*n)\n",
    "print((m*-1 + 1)*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_look = 100\n",
    "\n",
    "sub_graph1 = train_g.subgraph(selected_nodes[:num_look])\n",
    "sub_graph1.copy_from_parent()\n",
    "sub_graph1.register_message_func(m_func)\n",
    "sub_graph1.register_reduce_func(m_reduce_func)\n",
    "\n",
    "sub_graph2 = labeled_g.subgraph(out_nodes)\n",
    "sub_graph2.copy_from_parent()\n",
    "sub_graph2.register_message_func(m_func)\n",
    "sub_graph2.register_reduce_func(m_reduce_func)\n",
    "\n",
    "print(sub_graph2.ndata['features'])\n",
    "\n",
    "knn.fit(model(sub_graph2, sub_graph2.ndata['features']).cpu().detach().numpy(), sub_graph2.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_graph1.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_graph1, sub_graph1.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print(f\"total: {len(used_lab)}, accuracy of all nodes: {metrics.accuracy_score((sub_graph1.ndata['t_labels']).cpu().detach().numpy(), (sub_graph1.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n",
    "feats = sub_graph1.ndata['features']\n",
    "labs = sub_graph1.ndata['t_labels'] #true label\n",
    "output = model(sub_graph1, feats)\n",
    "print(output)\n",
    "print(labs)\n",
    "\n",
    "\n",
    "pos = defaultdict(list)\n",
    "cos = []\n",
    "cos2 = []\n",
    "\n",
    "for n in range(num_look):\n",
    "    pos[n] = (output[n][0].item(), output[n][1].item())\n",
    "    cos.append(sub_graph1.ndata['p_labels'][n].item())\n",
    "    cos2.append(sub_graph1.ndata['t_labels'][n].item())\n",
    "\n",
    "nx.draw_networkx_nodes(sub_graph1.to_networkx(), pos, node_color=cos2, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=150)\n",
    "nx.draw(sub_graph1.to_networkx(), pos, node_color=cos, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.any(th.isnan(sub_labeled_graph.ndata['features']))\n",
    "th.all(th.isfinite(sub_labeled_graph.ndata['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
