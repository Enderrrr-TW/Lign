{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.data.utils import load_graphs\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set gpu if available\n",
    "if th.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    #device = th.device(\"cuda\")\n",
    "    device = th.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    device = th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operation for neigbors\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "    \n",
    "#gcn layer in network\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.pull(g.nodes())\n",
    "        g.apply_nodes(self.apply_mod)\n",
    "        \n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "#network\n",
    "class LIGN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(LIGN, self).__init__()\n",
    "        self.gcn1 = GCN(in_feats, 100, F.relu)\n",
    "        self.gcn2 = GCN(100, 30, F.relu)\n",
    "        self.gcn3 = GCN(30, out_feats, th.tanh)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        \n",
    "        return self.gcn3(g, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "\n",
    "def similarity_matrix(x): #pairwise distance\n",
    "    \"\"\"x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y = x\n",
    "    y_norm = x_norm.view(1, -1)\n",
    "\n",
    "    dist = x_norm + y_norm - 2.0 * th.mm(x, th.transpose(y, 0, 1))\"\"\"\n",
    "    n = x.size(0)\n",
    "    d = x.size(1)\n",
    "\n",
    "    y = x.unsqueeze(0).expand(n, n, d)\n",
    "    x = x.unsqueeze(1).expand(n, n, d)\n",
    "    \n",
    "    dist = th.pow(x - y, 2).sum(2)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def same_label(y):\n",
    "    s = y.size(0)\n",
    "    y_expand = y.unsqueeze(0).expand(s, s)\n",
    "    Y = y_expand.eq(y_expand.t())\n",
    "    return Y\n",
    "\n",
    "def pairwaise_loss(output, labels):\n",
    "    \"\"\"\n",
    "    if nodes with the same label: x^2\n",
    "    if nodes with different label: -x^2\n",
    "    \"\"\"\n",
    "    sim = similarity_matrix(output)\n",
    "    temp = same_label(labels)\n",
    "    temp_inv = temp*(-1) + 1\n",
    "    \n",
    "    same_l = (temp * sim)\n",
    "    same_l_inv = (temp_inv * sim)\n",
    "    loss = (th.sum(same_l)/th.sum(temp))**2 - (th.sum(same_l_inv)/th.sum(temp_inv))**2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def has_1_rand_label(nodes): return (nodes.data['t_labels'] == possible_lab[0]).squeeze(0)\n",
    "\n",
    "def filter_knn_label(nodes): return (nodes.data['t_labels'] == curr_used_lab).squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glist, label_dict = load_graphs(\"mnist/graph.bin\")\n",
    "\n",
    "ds_g = glist[0]\n",
    "\n",
    "ds_g.ndata['features'] = th.FloatTensor(ds_g.ndata['features']).to(device)\n",
    "ds_g.ndata['t_labels'] = th.LongTensor(ds_g.ndata['t_labels']).to(device)\n",
    "\n",
    "# to coordinate sending of features over the graph network\n",
    "m_func = fn.copy_src(src='h', out='m')\n",
    "m_reduce_func = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Hyper-parameters & Model ############\n",
    "\n",
    "# parameters\n",
    "DIST_VEC_SIZE = 10\n",
    "NUMBER_OF_LABELS = th.unique(ds_g.ndata[\"t_labels\"]).size(0)\n",
    "k_NEIGHBOR = 3\n",
    "EPOCH = 50\n",
    "INIT_NUM_LAB = 6 #must be at least 1\n",
    "LABEL_NODES = k_NEIGHBOR+10\n",
    "Lambda = 0.0001\n",
    "\n",
    "model = LIGN(ds_g.ndata['features'].size()[1], DIST_VEC_SIZE).to(device)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=k_NEIGHBOR)\n",
    "opt = th.optim.Adam(model.parameters(), lr=1e-3)# only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 6.00 GiB total capacity; 3.38 GiB already allocated; 232.47 MiB free; 4.40 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8242ae98bd03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 6.00 GiB total capacity; 3.38 GiB already allocated; 232.47 MiB free; 4.40 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "########### Supervised Learning ############\n",
    "th.cuda.empty_cache()\n",
    "train_g = ds_g.subgraph(ds_g.nodes()[:int(len(ds_g))])#100 percent of all the nodes\n",
    "train_g.copy_from_parent()\n",
    "\n",
    "labeled_nodes = th.LongTensor([])\n",
    "for i in range(NUMBER_OF_LABELS):\n",
    "    def has_feature_one(nodes): return (nodes.data['t_labels'] == i).squeeze(0)\n",
    "    \n",
    "    filt = train_g.filter_nodes(has_feature_one)\n",
    "    labeled_nodes = th.cat((filt[:LABEL_NODES], labeled_nodes), -1)\n",
    "    \n",
    "\n",
    "labeled_g = train_g.subgraph(labeled_nodes) #labels nodes in \n",
    "labeled_g.copy_from_parent()\n",
    "\n",
    "\n",
    "possible_lab = list(range(NUMBER_OF_LABELS))\n",
    "rand.shuffle(possible_lab) #set random order for how labels are added\n",
    "\n",
    "used_lab = []\n",
    "curr_used_lab = 0\n",
    "\n",
    "EPOCH_SIZE = 0\n",
    "model.train()\n",
    "\n",
    "#############train with 2 labels of 7 labels (uses true label from the dataset), assign vector to each node,\n",
    "            #knn (1 labeled node for each label), test\n",
    "selected_nodes = train_g.filter_nodes(has_1_rand_label) #add two unseen labels\n",
    "used_lab.append(possible_lab.pop(0))\n",
    "for i in range(INIT_NUM_LAB - 1):\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "\n",
    "c=th.randperm(len(selected_nodes)) #shuffle\n",
    "selected_nodes=selected_nodes[c]\n",
    "\n",
    "EPOCH_SIZE = int(len(selected_nodes)*.80)\n",
    "\n",
    "#train\n",
    "addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer\n",
    "error = []\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"> \" + str(epoch))\n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    epoch_nodes = selected_nodes[:EPOCH_SIZE]\n",
    "    \n",
    "    sub_graph = train_g.subgraph(epoch_nodes)\n",
    "    sub_graph.copy_from_parent()\n",
    "    sub_graph.register_message_func(m_func)\n",
    "    sub_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    feats = sub_graph.ndata['features']\n",
    "    labs = sub_graph.ndata['t_labels'] #true label\n",
    "\n",
    "    out = model(sub_graph, feats)\n",
    "\n",
    "    out2 = addon_layer(sub_graph, out) #extra layer\n",
    "    out2 = th.log_softmax(out2, 1)\n",
    "\n",
    "    loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "    error.append(loss.item())\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    th.cuda.empty_cache()\n",
    "\n",
    "print(error)\n",
    "\n",
    "    \n",
    "#knn and assign vector\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy())\n",
    "print((sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())\n",
    "\n",
    "#test\n",
    "print(f\"> Initial training with {INIT_NUM_LAB} labels (distinction efficiency): {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After new label added and before retraining (total: 7), accuracy of all nodes: 0.4055508354573775\n",
      "> 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 6.00 GiB total capacity; 3.29 GiB already allocated; 298.47 MiB free; 4.34 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-16777b7d7c45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 6.00 GiB total capacity; 3.29 GiB already allocated; 298.47 MiB free; 4.34 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "  \n",
    "########### Unsupervised Learning ############\n",
    "\n",
    "#######################add label, assign vector to each node, knn, test and train (with predicted labels by knn)\n",
    "\n",
    "while len(possible_lab) > 0: #do it for every unseen label\n",
    "    selected_nodes = th.cat((train_g.filter_nodes(has_1_rand_label), selected_nodes), -1)\n",
    "    used_lab.append(possible_lab.pop(0))\n",
    "    \n",
    "    c=th.randperm(len(selected_nodes)) #shuffle\n",
    "    selected_nodes=selected_nodes[c]\n",
    "    \n",
    "    #assign vector\n",
    "    \n",
    "    #########\n",
    "    sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "    sub_tra_graph.copy_from_parent()\n",
    "    sub_tra_graph.register_message_func(m_func)\n",
    "    sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    ##labeled graph knn\n",
    "    out_nodes = th.LongTensor([])\n",
    "    for n in used_lab:\n",
    "        curr_used_lab = n\n",
    "        out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "    c=th.randperm(len(out_nodes)) #shuffle\n",
    "    out_nodes=out_nodes[c]\n",
    "\n",
    "    sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "    sub_labeled_graph.copy_from_parent()\n",
    "    sub_labeled_graph.register_message_func(m_func)\n",
    "    sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "    knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "    sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "    train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "    \n",
    "    #test\n",
    "    print(f\"After new label added and before retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "    #########\n",
    "    \n",
    "    addon_layer = GCN(DIST_VEC_SIZE, NUMBER_OF_LABELS, None).to(device) #extra layer\n",
    "    error = []\n",
    "    for epoch in range(EPOCH):\n",
    "        print(\"> \" + str(epoch))\n",
    "        c=th.randperm(len(selected_nodes)) #shuffle\n",
    "        selected_nodes=selected_nodes[c]\n",
    "        epoch_nodes = selected_nodes[:EPOCH_SIZE] #select 20% of random nodes to train with at each epoch\n",
    "\n",
    "        sub_graph = train_g.subgraph(epoch_nodes)\n",
    "        sub_graph.copy_from_parent()\n",
    "        sub_graph.register_message_func(m_func)\n",
    "        sub_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "        feats = sub_graph.ndata['features']\n",
    "        labs = sub_graph.ndata['p_labels'] #true label\n",
    "\n",
    "        out = model(sub_graph, feats)\n",
    "        out2 = addon_layer(sub_graph, out) #extra layer\n",
    "        out2 = th.log_softmax(out2, 1)\n",
    "\n",
    "        loss = Lambda*pairwaise_loss(out, labs) + F.nll_loss(out2, labs)\n",
    "        error.append(loss.item())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        th.cuda.empty_cache()\n",
    "print(error)\n",
    "\n",
    "sub_tra_graph = train_g.subgraph(selected_nodes)\n",
    "sub_tra_graph.copy_from_parent()\n",
    "sub_tra_graph.register_message_func(m_func)\n",
    "sub_tra_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "##labeled graph knn\n",
    "out_nodes = th.LongTensor([])\n",
    "for n in used_lab:\n",
    "    curr_used_lab = n\n",
    "    out_nodes = th.cat((labeled_g.filter_nodes(filter_knn_label), out_nodes), -1)\n",
    "\n",
    "c=th.randperm(len(out_nodes)) #shuffle\n",
    "out_nodes=out_nodes[c]\n",
    "\n",
    "sub_labeled_graph = labeled_g.subgraph(out_nodes)\n",
    "sub_labeled_graph.copy_from_parent()\n",
    "sub_labeled_graph.register_message_func(m_func)\n",
    "sub_labeled_graph.register_reduce_func(m_reduce_func)\n",
    "\n",
    "knn.fit(model(sub_labeled_graph, sub_labeled_graph.ndata['features']).cpu().detach().numpy(), sub_labeled_graph.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_tra_graph.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_tra_graph, sub_tra_graph.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "train_g.nodes[sub_tra_graph.parent_nid].data['p_labels'] = sub_tra_graph.ndata['p_labels']\n",
    "\n",
    "#test\n",
    "print(f\"After new label added and after retraining (total: {len(used_lab)}), accuracy of all nodes: {metrics.accuracy_score((sub_tra_graph.ndata['t_labels']).cpu().detach().numpy(), (sub_tra_graph.ndata['p_labels']).detach().cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "total: 10, accuracy of all nodes: 0.51\n",
      "tensor([[-1.0000e+00, -9.9954e-01,  1.0000e+00,  1.0000e+00, -9.9991e-01,\n",
      "         -1.0000e+00,  9.9991e-01, -1.0000e+00,  9.9781e-01,  9.9869e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9997e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-9.9590e-01, -1.0000e+00,  9.9997e-01,  1.0000e+00,  3.1492e-01,\n",
      "         -1.0000e+00, -9.6004e-01,  9.1758e-01, -9.8836e-01,  9.9992e-01],\n",
      "        [-1.0000e+00,  7.5773e-01,  1.0000e+00,  1.0000e+00, -9.9985e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9999e-01,  9.9268e-01],\n",
      "        [-1.0000e+00, -9.9973e-01,  1.0000e+00,  1.0000e+00, -9.9901e-01,\n",
      "         -1.0000e+00,  9.9477e-01, -9.9995e-01,  9.5584e-01,  9.9184e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  9.9995e-01, -1.0000e+00,  9.9660e-01,  9.9970e-01],\n",
      "        [-1.0000e+00, -9.9967e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9998e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9999e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9997e-01,  9.9999e-01],\n",
      "        [-1.0000e+00, -6.3567e-01,  1.0000e+00,  1.0000e+00, -9.9939e-01,\n",
      "         -1.0000e+00,  9.9998e-01, -9.9998e-01,  9.9974e-01,  9.9553e-01],\n",
      "        [-1.0000e+00, -9.9987e-01,  1.0000e+00,  1.0000e+00, -9.9994e-01,\n",
      "         -1.0000e+00,  9.9984e-01, -1.0000e+00,  9.9584e-01,  9.9920e-01],\n",
      "        [-9.9751e-01, -1.0000e+00,  9.9999e-01,  1.0000e+00, -1.4170e-01,\n",
      "         -1.0000e+00, -1.0000e+00,  9.9603e-01, -9.9998e-01,  9.9958e-01],\n",
      "        [-1.0000e+00,  7.5990e-01,  1.0000e+00,  1.0000e+00, -9.7484e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9999e-01,  9.9999e-01,  9.9915e-01],\n",
      "        [-1.0000e+00, -9.9999e-01,  1.0000e+00,  1.0000e+00, -9.9995e-01,\n",
      "         -1.0000e+00,  9.9879e-01, -1.0000e+00,  9.7033e-01,  9.9869e-01],\n",
      "        [-8.5885e-01, -9.9998e-01,  9.9728e-01,  9.9839e-01,  8.1113e-01,\n",
      "         -9.9658e-01, -9.9779e-01,  9.4105e-01, -9.6651e-01,  9.9219e-01],\n",
      "        [-1.0000e+00, -9.9740e-01,  1.0000e+00,  1.0000e+00, -9.9969e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-9.9968e-01, -9.9998e-01,  9.9999e-01,  1.0000e+00, -4.5455e-01,\n",
      "         -1.0000e+00, -7.1956e-01, -8.9663e-01, -4.0896e-01,  9.9591e-01],\n",
      "        [-9.9998e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  9.9495e-02,\n",
      "         -1.0000e+00, -8.7430e-01, -9.8469e-01, -6.2029e-01,  9.9998e-01],\n",
      "        [-1.0000e+00,  9.8571e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9992e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9600e-01,\n",
      "         -1.0000e+00,  9.9990e-01, -1.0000e+00,  9.9971e-01,  9.9999e-01],\n",
      "        [-1.0000e+00, -5.8874e-01,  1.0000e+00,  1.0000e+00, -9.6584e-01,\n",
      "         -1.0000e+00,  9.9997e-01, -9.9998e-01,  9.9993e-01,  9.9865e-01],\n",
      "        [-1.0000e+00,  6.6249e-01,  1.0000e+00,  1.0000e+00, -9.8129e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9998e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.8639e-01,\n",
      "         -1.0000e+00, -9.2830e-01, -9.8914e-01, -9.1258e-01,  9.7260e-01],\n",
      "        [-9.9958e-01, -9.9982e-01,  9.9998e-01,  1.0000e+00, -7.2476e-01,\n",
      "         -1.0000e+00, -4.8163e-01, -9.6573e-01, -4.5006e-01,  9.7447e-01],\n",
      "        [-1.0000e+00, -9.9994e-01,  1.0000e+00,  1.0000e+00, -9.9796e-01,\n",
      "         -1.0000e+00,  9.9997e-01, -1.0000e+00,  9.9991e-01,  9.9995e-01],\n",
      "        [-1.0000e+00,  6.0097e-01,  1.0000e+00,  1.0000e+00, -9.9996e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9988e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00,  1.7478e-02,\n",
      "         -1.0000e+00, -1.0766e-01, -9.9992e-01,  9.8903e-01,  9.9981e-01],\n",
      "        [-9.9867e-01, -9.9964e-01,  9.9997e-01,  1.0000e+00,  2.1317e-01,\n",
      "         -1.0000e+00,  4.2272e-01, -5.0239e-01,  1.3344e-01,  9.9979e-01],\n",
      "        [-1.0000e+00, -8.7209e-01,  1.0000e+00,  1.0000e+00, -9.9882e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9999e-01,  9.9934e-01],\n",
      "        [-9.9979e-01, -9.9993e-01,  9.9999e-01,  1.0000e+00, -9.7283e-01,\n",
      "         -1.0000e+00,  4.6279e-02, -8.5878e-01, -6.7795e-01,  9.7765e-01],\n",
      "        [-9.9999e-01, -9.8471e-01,  1.0000e+00,  1.0000e+00, -8.2815e-01,\n",
      "         -1.0000e+00,  9.9787e-01, -9.9989e-01,  9.9689e-01,  9.9804e-01],\n",
      "        [-8.8960e-01, -9.9999e-01,  9.9413e-01,  9.9380e-01,  3.1089e-01,\n",
      "         -9.9661e-01, -9.9940e-01,  9.7855e-01, -9.9735e-01,  9.7528e-01],\n",
      "        [-9.9995e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  5.9596e-01,\n",
      "         -1.0000e+00, -5.2660e-01, -9.9805e-01,  9.6936e-01,  9.9578e-01],\n",
      "        [-1.0000e+00, -9.9539e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-1.0000e+00, -9.9998e-01,  1.0000e+00,  1.0000e+00, -9.9991e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-1.0000e+00,  7.6888e-01,  1.0000e+00,  1.0000e+00, -9.9853e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9997e-01],\n",
      "        [-9.9999e-01, -9.9877e-01,  1.0000e+00,  1.0000e+00,  2.6223e-01,\n",
      "         -1.0000e+00,  9.9990e-01, -9.9997e-01,  9.9996e-01,  1.0000e+00],\n",
      "        [-9.9976e-01, -9.9998e-01,  9.9999e-01,  1.0000e+00, -1.1478e-02,\n",
      "         -1.0000e+00, -8.0595e-01, -9.3395e-01,  1.3823e-01,  9.9420e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9967e-01,  9.9994e-01],\n",
      "        [-1.0000e+00, -9.9924e-01,  1.0000e+00,  1.0000e+00, -9.9867e-01,\n",
      "         -1.0000e+00,  9.9995e-01, -1.0000e+00,  9.9986e-01,  9.9926e-01],\n",
      "        [-9.9880e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00, -6.6747e-01,\n",
      "         -1.0000e+00, -9.9983e-01,  9.8098e-01, -9.9991e-01,  9.9944e-01],\n",
      "        [-1.0000e+00,  7.8501e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-8.2365e-01, -9.9999e-01,  9.9215e-01,  9.9584e-01, -4.5373e-02,\n",
      "         -9.9874e-01, -9.9974e-01,  9.9795e-01, -9.9973e-01,  9.8742e-01],\n",
      "        [-1.0000e+00, -9.9984e-01,  1.0000e+00,  1.0000e+00, -9.9328e-01,\n",
      "         -1.0000e+00,  9.9992e-01, -1.0000e+00,  9.9990e-01,  9.9976e-01],\n",
      "        [-1.0000e+00,  9.6356e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9935e-01],\n",
      "        [-1.0000e+00,  8.3982e-01,  1.0000e+00,  1.0000e+00, -9.9958e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9999e-01,  9.9991e-01,  9.9183e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9782e-01,\n",
      "         -1.0000e+00,  9.0244e-01, -9.9999e-01,  8.0416e-01,  9.9899e-01],\n",
      "        [-9.8895e-01, -9.9995e-01,  9.9991e-01,  9.9989e-01,  9.3542e-01,\n",
      "         -9.9811e-01, -9.3825e-01, -7.7999e-01,  5.1546e-01,  9.8923e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9999e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-9.9886e-01, -9.9714e-01,  9.9995e-01,  1.0000e+00, -1.0120e-01,\n",
      "         -9.9996e-01,  7.0688e-01, -9.9323e-01,  9.2518e-01,  9.7784e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.5893e-01,\n",
      "         -1.0000e+00, -9.9202e-01, -9.8526e-01, -9.8474e-01,  9.9843e-01],\n",
      "        [-9.9994e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  8.8408e-01,\n",
      "         -1.0000e+00, -9.3657e-01, -9.9452e-01,  9.1300e-01,  9.9786e-01],\n",
      "        [-1.0000e+00,  9.9998e-01,  1.0000e+00,  1.0000e+00,  6.8396e-04,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9983e-01,  1.0000e+00,  1.0000e+00],\n",
      "        [-1.0000e+00,  8.8277e-01,  1.0000e+00,  1.0000e+00, -9.9987e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9998e-01,  9.9212e-01],\n",
      "        [-1.0000e+00, -9.1848e-01,  1.0000e+00,  1.0000e+00, -9.9817e-01,\n",
      "         -1.0000e+00,  9.9997e-01, -1.0000e+00,  9.9983e-01,  9.9693e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9848e-01,\n",
      "         -1.0000e+00,  9.9931e-01, -1.0000e+00,  9.9885e-01,  9.9998e-01],\n",
      "        [-9.9846e-01,  9.9507e-01,  9.9994e-01,  1.0000e+00,  9.1052e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9135e-01,  9.9999e-01,  9.9998e-01],\n",
      "        [-1.0000e+00, -9.9930e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-1.0000e+00,  9.7793e-01,  1.0000e+00,  1.0000e+00, -7.2943e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9992e-01,  9.9999e-01,  9.9993e-01],\n",
      "        [-9.5195e-01, -9.9999e-01,  9.9835e-01,  9.9949e-01,  4.8775e-01,\n",
      "         -9.9929e-01, -9.9811e-01,  9.2050e-01, -9.8853e-01,  9.9132e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9937e-01,\n",
      "         -1.0000e+00,  9.9907e-01, -1.0000e+00,  9.9529e-01,  9.9989e-01],\n",
      "        [-1.0000e+00,  3.3641e-01,  1.0000e+00,  1.0000e+00, -9.5273e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9958e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -7.9786e-01,\n",
      "         -1.0000e+00, -5.5492e-02, -9.9919e-01,  7.1254e-01,  9.9988e-01],\n",
      "        [-1.0000e+00,  8.7747e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-1.0000e+00, -9.9999e-01,  1.0000e+00,  1.0000e+00, -9.9987e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9997e-01,  9.9999e-01],\n",
      "        [-1.0000e+00, -9.9853e-01,  1.0000e+00,  1.0000e+00, -9.9961e-01,\n",
      "         -1.0000e+00,  9.9996e-01, -1.0000e+00,  9.9958e-01,  9.9931e-01],\n",
      "        [-9.9991e-01, -9.9937e-01,  1.0000e+00,  1.0000e+00, -4.7443e-01,\n",
      "         -1.0000e+00,  9.1006e-01, -9.9835e-01,  9.4549e-01,  9.9736e-01],\n",
      "        [-9.9907e-01, -9.9998e-01,  1.0000e+00,  1.0000e+00,  7.8400e-01,\n",
      "         -1.0000e+00,  5.6947e-01, -8.9277e-01,  5.9560e-01,  9.9999e-01],\n",
      "        [-1.0000e+00,  9.7720e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9950e-01],\n",
      "        [-9.9999e-01, -2.3295e-01,  1.0000e+00,  1.0000e+00, -6.9170e-01,\n",
      "         -1.0000e+00,  9.9997e-01, -9.9982e-01,  9.9990e-01,  9.9985e-01],\n",
      "        [-9.9999e-01,  5.8107e-01,  1.0000e+00,  1.0000e+00,  7.6082e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9789e-01,  9.9999e-01,  1.0000e+00],\n",
      "        [-9.7761e-01, -9.9409e-01,  9.9971e-01,  1.0000e+00,  9.5376e-01,\n",
      "         -9.9999e-01,  7.9904e-01,  2.0358e-01,  7.7019e-01,  9.9996e-01],\n",
      "        [-1.0000e+00, -1.2527e-01,  1.0000e+00,  1.0000e+00, -9.9993e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-9.9999e-01,  5.8568e-01,  1.0000e+00,  1.0000e+00, -8.5905e-01,\n",
      "         -1.0000e+00,  9.9994e-01, -9.9983e-01,  9.9978e-01,  9.9729e-01],\n",
      "        [-1.0000e+00, -9.7383e-01,  1.0000e+00,  1.0000e+00, -9.9999e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9996e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9994e-01,\n",
      "         -1.0000e+00,  8.4100e-01, -1.0000e+00,  4.3734e-01,  9.9973e-01],\n",
      "        [-1.0000e+00, -9.9925e-01,  1.0000e+00,  1.0000e+00, -9.9775e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9998e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9576e-01,\n",
      "         -1.0000e+00,  8.3707e-01, -9.9998e-01,  9.0406e-01,  9.9886e-01],\n",
      "        [-9.9998e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  3.8377e-01,\n",
      "         -1.0000e+00, -1.4769e-01, -9.9849e-01,  9.5627e-01,  9.9848e-01],\n",
      "        [-9.9770e-01, -9.9978e-01,  9.9999e-01,  9.9999e-01,  8.8830e-01,\n",
      "         -9.9954e-01, -2.4014e-01, -9.8854e-01,  9.6734e-01,  9.8725e-01],\n",
      "        [-1.0000e+00, -3.3986e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00, -9.9996e-01,\n",
      "         -1.0000e+00,  9.9986e-01, -1.0000e+00,  9.9946e-01,  1.0000e+00],\n",
      "        [-1.0000e+00,  3.5172e-01,  1.0000e+00,  1.0000e+00, -9.9999e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9984e-01],\n",
      "        [-1.0000e+00,  9.2269e-01,  1.0000e+00,  1.0000e+00, -9.0907e-01,\n",
      "         -1.0000e+00,  9.9999e-01, -9.9991e-01,  9.9996e-01,  9.9923e-01],\n",
      "        [-1.0000e+00, -9.9294e-01,  1.0000e+00,  1.0000e+00, -9.9857e-01,\n",
      "         -1.0000e+00,  9.9999e-01, -1.0000e+00,  9.9997e-01,  9.9950e-01],\n",
      "        [-9.9818e-01, -9.9797e-01,  9.9996e-01,  1.0000e+00,  6.2700e-01,\n",
      "         -1.0000e+00,  7.2349e-01, -3.0237e-01,  5.1459e-01,  9.9990e-01],\n",
      "        [-1.0000e+00, -9.7335e-01,  1.0000e+00,  1.0000e+00, -9.9522e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
      "        [-1.0000e+00,  3.1953e-01,  1.0000e+00,  1.0000e+00, -8.3065e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -9.9999e-01,  1.0000e+00,  9.9983e-01],\n",
      "        [-9.9996e-01,  7.8528e-01,  1.0000e+00,  1.0000e+00,  4.7773e-01,\n",
      "         -1.0000e+00,  9.9999e-01, -9.9971e-01,  9.9997e-01,  9.9999e-01],\n",
      "        [-1.0000e+00,  7.3160e-01,  1.0000e+00,  1.0000e+00, -9.9999e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9940e-01],\n",
      "        [-1.0000e+00,  9.5611e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9924e-01],\n",
      "        [-1.0000e+00, -9.9930e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-1.0000e+00, -7.7767e-01,  1.0000e+00,  1.0000e+00, -9.9107e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9997e-01],\n",
      "        [-1.0000e+00, -9.9923e-01,  1.0000e+00,  1.0000e+00, -9.9991e-01,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9997e-01,  9.9969e-01],\n",
      "        [-9.9658e-01, -9.9998e-01,  9.9999e-01,  9.9999e-01,  9.7947e-01,\n",
      "         -9.9923e-01, -8.8701e-01, -9.6504e-01,  9.1878e-01,  9.9360e-01],\n",
      "        [-1.0000e+00, -9.9951e-01,  1.0000e+00,  1.0000e+00, -9.9603e-01,\n",
      "         -1.0000e+00,  9.9988e-01, -1.0000e+00,  9.9977e-01,  9.9904e-01],\n",
      "        [-9.9999e-01, -1.0000e+00,  1.0000e+00,  1.0000e+00,  8.3804e-01,\n",
      "         -1.0000e+00,  2.8145e-01, -9.9958e-01,  9.9485e-01,  9.9959e-01],\n",
      "        [-1.0000e+00, -9.9998e-01,  1.0000e+00,  1.0000e+00, -8.3632e-01,\n",
      "         -1.0000e+00,  9.9572e-01, -9.9999e-01,  9.9679e-01,  9.9994e-01],\n",
      "        [-1.0000e+00,  9.4371e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  1.0000e+00,  9.9985e-01],\n",
      "        [-9.9845e-01, -1.0000e+00,  9.9999e-01,  1.0000e+00, -8.2062e-01,\n",
      "         -1.0000e+00, -9.9999e-01,  9.9784e-01, -9.9999e-01,  9.9888e-01],\n",
      "        [-1.0000e+00, -9.8882e-01,  1.0000e+00,  1.0000e+00, -1.0000e+00,\n",
      "         -1.0000e+00,  1.0000e+00, -1.0000e+00,  9.9998e-01,  9.9971e-01]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward>)\n",
      "tensor([0, 0, 7, 6, 3, 2, 3, 0, 0, 2, 7, 4, 3, 7, 5, 5, 5, 6, 0, 8, 4, 3, 5, 8,\n",
      "        4, 1, 9, 5, 7, 5, 7, 1, 0, 0, 4, 5, 8, 2, 0, 7, 2, 7, 0, 6, 6, 2, 1, 0,\n",
      "        1, 3, 1, 4, 6, 5, 3, 4, 0, 9, 7, 2, 7, 8, 0, 2, 0, 1, 3, 6, 9, 9, 9, 4,\n",
      "        4, 6, 3, 0, 8, 1, 1, 0, 0, 5, 9, 6, 9, 4, 4, 9, 6, 6, 0, 5, 0, 1, 5, 1,\n",
      "        2, 6, 7, 2], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-68e455e0da98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mcos2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_graph1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m't_labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_networkx_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_graph1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcos2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rainbow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_graph1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rainbow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_labels' is not defined"
     ]
    }
   ],
   "source": [
    "num_look = 100\n",
    "\n",
    "sub_graph1 = train_g.subgraph(selected_nodes[:num_look])\n",
    "sub_graph1.copy_from_parent()\n",
    "sub_graph1.register_message_func(m_func)\n",
    "sub_graph1.register_reduce_func(m_reduce_func)\n",
    "\n",
    "sub_graph2 = labeled_g.subgraph(out_nodes)\n",
    "sub_graph2.copy_from_parent()\n",
    "sub_graph2.register_message_func(m_func)\n",
    "sub_graph2.register_reduce_func(m_reduce_func)\n",
    "\n",
    "print(sub_graph2.ndata['features'])\n",
    "\n",
    "knn.fit(model(sub_graph2, sub_graph2.ndata['features']).cpu().detach().numpy(), sub_graph2.ndata['t_labels'].cpu().detach().numpy())\n",
    "sub_graph1.ndata['p_labels'] = th.LongTensor(knn.predict(model(sub_graph1, sub_graph1.ndata['features']).cpu().detach().numpy())).to(device)\n",
    "\n",
    "print(f\"total: {len(used_lab)}, accuracy of all nodes: {metrics.accuracy_score((sub_graph1.ndata['t_labels']).cpu().detach().numpy(), (sub_graph1.ndata['p_labels']).detach().cpu().numpy())}\")\n",
    "\n",
    "feats = sub_graph1.ndata['features']\n",
    "labs = sub_graph1.ndata['t_labels'] #true label\n",
    "output = model(sub_graph1, feats)\n",
    "print(output)\n",
    "print(labs)\n",
    "\n",
    "\n",
    "pos = defaultdict(list)\n",
    "cos = []\n",
    "cos2 = []\n",
    "\n",
    "for n in range(num_look):\n",
    "    pos[n] = (output[n][0].item(), output[n][1].item())\n",
    "    cos.append(sub_graph1.ndata['p_labels'][n].item())\n",
    "    cos2.append(sub_graph1.ndata['t_labels'][n].item())\n",
    "\n",
    "nx.draw_networkx_nodes(sub_graph1.to_networkx(), pos, node_color=cos2, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=150)\n",
    "nx.draw(sub_graph1.to_networkx(), pos, node_color=cos, cmap=plt.get_cmap('rainbow'), vmin = float(ds_labels.min().item()), vmax = float(ds_labels.max().item()), node_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
